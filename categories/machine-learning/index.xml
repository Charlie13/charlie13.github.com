<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on taewan.kim</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hidden Layer의 오차 계산하기</title>
      <link>/post/error_in_hidden/</link>
      <pubDate>Fri, 08 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/error_in_hidden/</guid>
      <description>그림 1: Neural Network 예제: L=2, 2 Layer Neural Network     그림 2: Input Feature를 이용한 Forward Propagation     그림 3: Layer 2에서 Forward Propagation     그림 4: Layer 2(출력층)의 출력을 이용한 오차 계산     그림 5: 출력층의 첫 번째 노드의 오차에 영향을 미치는 요소     그림 6: 출력층의 두 번째 노드의 오차에 영향을 미치는 요소     그림 7: Layer 2(출력층) 첫번째 노드 오차를 Layer 1에 분배     그림 8: Lyaer 1의 첫번째 노드 오차 계산하기     그림 9: Layer 1의 오차 계산 행렬식     그림 10: Layer 1의 오차 계산 단순화하기     그림 11: Layer 1의 오차 계산 결과    은닉층 오차 계산식 일반화 참고자료  Coursera: Neural Networks and Deep Learning by deeplearning.</description>
    </item>
    
    <item>
      <title>Neural Network 표기법(Feat: Andrew NG)</title>
      <link>/post/nn_notation/</link>
      <pubDate>Tue, 28 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_notation/</guid>
      <description>그림 1: Neural Network 예제    Neural Network에 대한 Forward Propagation, Backpropagation, predict, Cost Function 등을 정리할 때 뉴럴 네트워크의 구성 요소와 각 위치가 혼동되어 어려움을 겪는 경우가 많습니다. Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의에서 Neural Network 표기법 잘 정리하고 있습니다. Andrew NG 교수가 소개한 Neural Network 표기법을 정리합니다. 이 표기법을 사용하면 Neural Network의 여러 수식과 알고리즘을 다룰 때 혼동을 최소화 할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Sigmoid 함수 미분 정리</title>
      <link>/post/sigmoid_diff/</link>
      <pubDate>Mon, 18 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/sigmoid_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
  </channel>
</rss>