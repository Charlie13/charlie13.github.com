<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on taewan.kim</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Backpropagation를 위한 편미분 과정에서 행렬 Transpose의 발생 이유?</title>
      <link>/post/backpropagation_matrix_transpose/</link>
      <pubDate>Wed, 20 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/backpropagation_matrix_transpose/</guid>
      <description>Backpropagation을 직접 구현하는 과정에서 이유를 알 수 없는 갑작스런 행렬 Transpose와 관련된 의구심이 오랜기간 절 괴롭혔습니다. 그 의문은 Cost Function을 해당 레이어의 W(가중치)로 편미분하여 현재 W를 수정하는 식을 유도하는 과정에서 일부 행렬이 전치행렬로 갑지기 변경되는 이유에 대한 궁금증이었습니다. 행렬이 전치(Transpose)되는 근거와 이유를 알수 없다는 답답함 이었습니다. 딥러닝 책이나 웹 문서를 찾아보면 &amp;ldquo;편미분 과정에서 적당히 행렬을 맞춰준다.&amp;ldquo;라는 표현으로 이 부분을 대충 넘어가고 있습니다. Backpropagation 미분 과정에서 행렬의 방향성(Transpose 할 것이나 말것이냐)은 어떻게 결정할 것인가라는 질문이었습니다.</description>
    </item>
    
    <item>
      <title>딥러닝을 위한 Norm, 노름</title>
      <link>/post/norm/</link>
      <pubDate>Fri, 15 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/norm/</guid>
      <description>기계학습 자료에서 간혹 Norm과 관련된 수식이나 표기법을 나오면 당황스러울 때가 있습니다. 선형대수에 익숙하지 않다면 Norm이 이상하게 보일 수 있습니다. 본 문서에서는 인공신공망과 기계학습 일고리즘에서 사용되는 Norm을 이해하는 것을 목표로 최소한도의 Norm 개념을 정리합니다.
일반적으로 딥러닝에서 네트워크의 Overfitting(과적합) 문제를 해결하는 방법으로 다음과 같은 3가지 방법을 제시합니다.
 더 많은 데이터를 사용할 것 Cross Validation Regularization  더 이상 학습 데이터를 추가할 수 없거나 학습 데이터를 늘려도 과적합 문제가 해결되지 않을 때에는 3번 Regularization을 사용해야 합니다.</description>
    </item>
    
    <item>
      <title>Coursera의 deeplearning.ai Course2 강좌 1주차 정리</title>
      <link>/writing/deeplearning_course2_week1/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/deeplearning_course2_week1/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하고 있는 Deep Learning 강좌 &amp;ldquo;5-course Specialization&amp;ldquo;의 두 번째 과정인 &amp;ldquo;Improving Deep Neural Network&amp;ldquo;의 1주차 강의를 정리합니다. 강좌의 세부 정보는 다음과 같습니다.
0. 강의 소개 본 문서에서 정리하는 강의 정보는 다음과 같습니다.
0.1 강좌 세부 정보  Mock: Coursera 운영: deeplearning.ai 과정명: 5-course Specialization 코스명: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizatio 주차: Week 1 주제: Practical aspect of Deep Learning  0.2 1주차 강의 학습 목표 1주차 강의 학습 목표는 다음과 같습니다.</description>
    </item>
    
    <item>
      <title>딥러닝의 주요 활성화 함수(Activation function)</title>
      <link>/writing/activation_function_derivative/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/activation_function_derivative/</guid>
      <description>Artificial Neural Network (ANN, 인공 신경망) 모델을 구성 할 때 가장 중요한 결정사항은 레이어 갯수와 은닉층(Hidden Layer)과 출력층(Output Layer)에서 사용할 활성화 함수를 결정하는 것입니다. 인공 신경망에서 활성화 함수(Activation function)는 노드에 입력값에 대한 변환 함수입니다.
노드의 입력 데이터는 활성화 함수를 거쳐 출력 함수로 변환 됩니다. (&amp;lt;그림 1&amp;gt; 참조)
 그림 1: 인공 신경망에서 노드의 입력 값을 변환하는 활성화 함수    인공 신경망에서 활성화 함수가 필요한 이유 여러개의 레이어로 구성되어 있고, 이전 레이어의 출력은 다음 레이어의 입력이 되는 신경망에서 모든 레이어가 Activation 함수로 선형 함수를 사용한다면, 이 신경망은 1개의 레이어로 표현 가능합니다.</description>
    </item>
    
    <item>
      <title>Hidden Layer의 오차 계산</title>
      <link>/post/error_in_hidden/</link>
      <pubDate>Fri, 08 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/error_in_hidden/</guid>
      <description>Neural Network에서는 Forward Propagation 결과로 계산된 Output Layer 출력과 해당 입력 데이터 레이블의 차이를 계산하여 오차(손실, Error/Loss)를 계산합니다. 그리고 이 오차 최소화를 목표로 Hidden Layer들의 Weight(가중치)와 Bias(편향)를 업데이트합니다. 이렇게 딥러닝에서는 Neural Network에 데이터를 지속해서 흘려보내고, 오차를 계산한 후 Weight와 Bias를 수정하는 작업을 반복합니다.
Output Layer 출력과 레이블의 차이를 계산하고 은닉층의 Weight와 Bias를 업데이트하는 일련의 과정을 &amp;ldquo;역전파&amp;rdquo; 혹은 &amp;ldquo;Backpropagation&amp;ldquo;이라고 합니다. &amp;ldquo;Backpropagation&amp;ldquo;에서는 출력층의 오차로부터 은닉층의 노드별 오차를 계산하는 것이 핵심입니다. 은닉층의 노드별 오차를 알아야 은닉층의 Weight와 Bias를 수정할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Neural Network 표기법(Feat: Andrew NG)</title>
      <link>/post/nn_notation/</link>
      <pubDate>Tue, 28 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_notation/</guid>
      <description>그림 1: Neural Network 예제    Neural Network에 대한 Forward Propagation, Backpropagation, predict, Cost Function 등을 정리할 때 뉴럴 네트워크의 구성 요소와 각 위치가 혼동되어 어려움을 겪는 경우가 많습니다. Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의에서 Neural Network 표기법 잘 정리하고 있습니다. Andrew NG 교수가 소개한 Neural Network 표기법을 정리합니다. 이 표기법을 사용하면 Neural Network의 여러 수식과 알고리즘을 다룰 때 혼동을 최소화 할 수 있습니다.</description>
    </item>
    
    <item>
      <title>tanh 미분 정리</title>
      <link>/post/tanh_diff/</link>
      <pubDate>Tue, 19 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/tanh_diff/</guid>
      <description>Hyperbolic Tangent(tanh) 함수는 Sigmoid의 대체제로 사용될 수 있는 활성화 함수입니다. Hyperbolic Tangent(tanh)는 Sigmoid와 매우 유사합니다. 실제로, Hyperbolic Tangent 함수는 확장 된 시그모이드 함수입니다. tanh와 Sigmoid의 차이점은 Sigmoid의 출력 범위가 0에서 1 사이인 반면 tanh와 출력 범위는 -1에서 1사이라는 점입니다. Sigmoid와 비교하여 tanh와는 출력 범위가 더 넓고 경사면이 큰 범위가 더 크기 때문에 더 빠르게 수렴하여 학습하는 특성이 있습니다.
Sigmoid와 비교하여 중심점이 0이고 범위가 기울기 넓은 차이점이 있지만 Sigmoid의 치명적인 단점인 Vanishing gradient problem 문제를 그대로 갖고 있습니다.</description>
    </item>
    
    <item>
      <title>Sigmoid 함수 미분 정리</title>
      <link>/post/sigmoid_diff/</link>
      <pubDate>Mon, 18 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/sigmoid_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
    <item>
      <title>Logistic regression의 Cost Function과 미분</title>
      <link>/writing/logistic_regression_diff/</link>
      <pubDate>Fri, 30 Jun 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/logistic_regression_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
  </channel>
</rss>