<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>표기법 on taewan.kim</title>
    <link>/tags/%ED%91%9C%EA%B8%B0%EB%B2%95/</link>
    <description>Recent content in 표기법 on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Nov 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/%ED%91%9C%EA%B8%B0%EB%B2%95/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>신경망 Weight&amp;Bias Shape의 일반화</title>
      <link>/post/nn_matrix_demension/</link>
      <pubDate>Wed, 29 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_matrix_demension/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의(4주차: Getting your matrix dimensions)에서 다룬 &amp;ldquo;Neural Network의 Weight와 Bias Shape 계산 일반화&amp;ldquo;를 정리합니다.
layer 1의 연산 Input Feature로 단일 데이터(Single Instance)로 데이터가 유일될 경우 Layer 1에서는 다음과 같은 연산이 이루어 집니다.
 $$ \begin{align} Z^{[1]} &amp;amp; = W^{[1]}X + b^{[1]} \\
A^{[^1]} &amp;amp; = g^{[1]}(Z^{[1]}) \end{align} $$
 Neural Network의 레이어 표기법 Neural Netowk의 레이어 표기법은 Input Feature를 &amp;ldquo;Layer 0&amp;rdquo;로 표시합니다.</description>
    </item>
    
    <item>
      <title>Neural Network 표기법(Feat: Andrew NG)</title>
      <link>/post/nn_notation/</link>
      <pubDate>Tue, 28 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_notation/</guid>
      <description>그림 1: Neural Network 예제    Neural Network에 대한 Forward Propagation, Backpropagation, predict, Cost Function 등을 정리할 때 뉴럴 네트워크의 구성 요소와 각 위치가 혼동되어 어려움을 겪는 경우가 많습니다. Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의에서 Neural Network 표기법 잘 정리하고 있습니다. Andrew NG 교수가 소개한 Neural Network 표기법을 정리합니다. 이 표기법을 사용하면 Neural Network의 여러 수식과 알고리즘을 다룰 때 혼동을 최소화 할 수 있습니다.</description>
    </item>
    
  </channel>
</rss>