<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on taewan.kim</title>
    <link>/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hidden Layer의 오차 계산</title>
      <link>/post/error_in_hidden/</link>
      <pubDate>Fri, 08 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/error_in_hidden/</guid>
      <description>Neural Network에서는 Forward Propagation의 결과로 계산된 출력층의 출력과 해당 데이터의 레이블을의 차이를 계산하여 오차(손실, Error/Loss)를 계산합니다. 그리고 이 오차가 최소화 되도록 Hidden Layer의 Weight와 Bias를 업데이트 한 후, 네트워크에 데이터를 지속적으로 흘려보내고, 오차를 계산한 후 Weight와 Bias를 수정하는 작업을 반복합니다.
출력층의 출력과 레이블의 차이를 계산하고 은닉층의 Wegiht와 Bias를 업데이트하는 일련의 과정을 &amp;ldquo;역전파&amp;rdquo; 혹은 &amp;ldquo;Backpropagation&amp;ldquo;이라고 합니다. &amp;ldquo;Backpropagation&amp;ldquo;에서는 출력층의 오차로 부터 은닉층의 노드별 오차를 계산하는 것이 핵심입니다. 은닉층의 노드별 오차를 알아야 은닉층의 Wegiht와 Bias를 수정할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Hidden Layer의 오차 계산</title>
      <link>/post/error_in_hidden/</link>
      <pubDate>Fri, 08 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/error_in_hidden/</guid>
      <description>Neural Network에서는 Forward Propagation의 결과로 계산된 출력층의 출력과 해당 데이터의 레이블을의 차이를 계산하여 오차(손실, Error/Loss)를 계산합니다. 그리고 이 오차가 최소화 되도록 Hidden Layer의 Weight와 Bias를 업데이트 한 후, 네트워크에 데이터를 지속적으로 흘려보내고, 오차를 계산한 후 Weight와 Bias를 수정하는 작업을 반복합니다.
출력층의 출력과 레이블의 차이를 계산하고 은닉층의 Wegiht와 Bias를 업데이트하는 일련의 과정을 &amp;ldquo;역전파&amp;rdquo; 혹은 &amp;ldquo;Backpropagation&amp;ldquo;이라고 합니다. &amp;ldquo;Backpropagation&amp;ldquo;에서는 출력층의 오차로 부터 은닉층의 노드별 오차를 계산하는 것이 핵심입니다. 은닉층의 노드별 오차를 알아야 은닉층의 Wegiht와 Bias를 수정할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Neural Network 표기법(Feat: Andrew NG)</title>
      <link>/post/nn_notation/</link>
      <pubDate>Tue, 28 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_notation/</guid>
      <description>그림 1: Neural Network 예제    Neural Network에 대한 Forward Propagation, Backpropagation, predict, Cost Function 등을 정리할 때 뉴럴 네트워크의 구성 요소와 각 위치가 혼동되어 어려움을 겪는 경우가 많습니다. Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의에서 Neural Network 표기법 잘 정리하고 있습니다. Andrew NG 교수가 소개한 Neural Network 표기법을 정리합니다. 이 표기법을 사용하면 Neural Network의 여러 수식과 알고리즘을 다룰 때 혼동을 최소화 할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Sigmoid 함수 미분 정리</title>
      <link>/post/sigmoid_diff/</link>
      <pubDate>Mon, 18 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/sigmoid_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
  </channel>
</rss>