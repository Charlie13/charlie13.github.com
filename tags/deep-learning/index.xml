<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on taewan.kim</title>
    <link>/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Nov 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>신경망 Weight&amp;Bias Shape의 일반화</title>
      <link>/post/backprop_formula/</link>
      <pubDate>Wed, 29 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/backprop_formula/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의(4주차: Getting your matrix dimensions)에서 다룬 &amp;ldquo;Neural Network의 Weight와 Bias Shape 계산 일반화&amp;ldquo;를 정리합니다.
Forward Propagation L개의 레이어로 구성된 신경망의 Forward Propagation은 다음과 같이 유도할 수 있습니다.
 $$ \begin{align} Z^{[1]} &amp;amp; = W^{[1]}X + b^{[2]} \\
A^{[^1]} &amp;amp; = g^{[1]}(Z^{[1]}) \\
Z^{[2]} &amp;amp; = W^{[2]}A^{[1]}X + b^{[2]} \\
A^{[^2]} &amp;amp; = g^{[2]}(Z^{[2]}) \\
&amp;amp; &amp;hellip;.. \\
Z^{[l]} &amp;amp; = W^{[l]}A^{[l-1]}X + b^{[2]} \\</description>
    </item>
    
    <item>
      <title>신경망 Weight&amp;Bias Shape의 일반화</title>
      <link>/post/nn_matrix_demension/</link>
      <pubDate>Wed, 29 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_matrix_demension/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의(4주차: Getting your matrix dimensions)에서 다룬 &amp;ldquo;Neural Network의 Weight와 Bias Shape 계산 일반화&amp;ldquo;를 정리합니다.
layer 1의 연산 Input Feature로 단일 데이터(Single Instance)로 데이터가 유일될 경우 Layer 1에서는 다음과 같은 연산이 이루어 집니다.
 $$ \begin{align} Z^{[1]} &amp;amp; = W^{[1]}X + b^{[1]} \\
A^{[^1]} &amp;amp; = g^{[1]}(Z^{[1]}) \end{align} $$
 Neural Network의 레이어 표기법 Neural Netowk의 레이어 표기법은 Input Feature를 &amp;ldquo;Layer 0&amp;rdquo;로 표시합니다.</description>
    </item>
    
    <item>
      <title>Neural Network 표기법(Feat: Andrew NG)</title>
      <link>/post/nn_notation/</link>
      <pubDate>Tue, 28 Nov 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/nn_notation/</guid>
      <description>그림 1: Neural Network 예제    Neural Network에 대한 Forward Propagation, Backpropagation, predict, Cost Function 등을 정리할 때 뉴럴 네트워크의 구성 요소와 각 위치가 혼동되어 어려움을 겪는 경우가 많습니다. Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의에서 Neural Network 표기법 잘 정리하고 있습니다. Andrew NG 교수가 소개한 Neural Network 표기법을 정리합니다. 이 표기법을 사용하면 Neural Network의 여러 수식과 알고리즘을 다룰 때 혼동을 최소화 할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Sigmoid 함수 미분 정리</title>
      <link>/post/sigmoid_diff/</link>
      <pubDate>Mon, 18 Sep 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/sigmoid_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
    <item>
      <title>Neural Network의 Backward Propagation의 일반화</title>
      <link>/post/backpropagation/</link>
      <pubDate>Mon, 01 Dec 2014 21:28:14 +0900</pubDate>
      
      <guid>/post/backpropagation/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하는 Neural Networks and Deep Learning 강의(4주차: Forward and Backward Propagation)에서 다룬 &amp;ldquo;Neural Network의 Forward Propagation과 Backward Propagation 일반화&amp;ldquo;를 정리합니다.
Neural Network에서는 Forward Propagation을 이용하여 입력한 데이터의 추정결과를 계산합니다. 지도학습에서는 입력 데이터의 추정결과와 해당 데이터의 실제 정답인 레이블(Label)과의 오차를 계산합니다. 이때 &amp;ldquo;Error Function/Loss Function&amp;ldquo;을 사용하여 추정결과와 레이블의 오차를 계산합니다. &amp;ldquo;Error Function/Loss Function&amp;ldquo;을 각 레이어의 출력값과 Weight, Bias로 미분하여 각 레이어의 Weight 와 Bias가 오차에 미치는 영향도를 계산합니다. 이 결과를 각 레이어의 Weight와 Bias에 반영(업데이트) 합니다.</description>
    </item>
    
  </channel>
</rss>