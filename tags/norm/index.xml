<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Norm on taewan.kim</title>
    <link>/tags/norm/</link>
    <description>Recent content in Norm on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/norm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>딥러닝을 위한 그만큼 노름, Norm</title>
      <link>/post/norm/</link>
      <pubDate>Fri, 15 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/norm/</guid>
      <description>딥러닝에서 네트워크의 Overfitting(과적합) 문제를 해결하는 방법으로 3가지를 제시합니다.
 더 많은 데이터를 사용할 것 Cross Validation Regularization  이 중에서 Regularization을 할 경우 Lose 함수를 다음과 같이 표현됩니다.
 $$ L&amp;rsquo; = L(W, b) + \lambda\frac{1}{2}||w||^2 $$ 
 위 수식은 기존 Lose 함수에 L2 Regularization을 위한 항을 추가한 새로운 Lose 함수입니다.
L2 Regularization을 위해서 추가한 새로운 항은 L2 Norm을 사용하고 있습니다. 딥러닝의 Regularization, kNN 알고리즘, kmean 알고리즘 등에서 L1 Norm/L2 Norm을 사용합니다.</description>
    </item>
    
    <item>
      <title>딥러닝의 주요 활성화 함수(Activation function)</title>
      <link>/writing/activation_function_derivative/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/activation_function_derivative/</guid>
      <description>Artificial Neural Network (ANN, 인공 신경망) 모델을 구성 할 때 가장 중요한 결정사항은 레이어 갯수와 은닉층(Hidden Layer)과 출력층(Output Layer)에서 사용할 활성화 함수를 결정하는 것입니다. 인공 신경망에서 활성화 함수(Activation function)는 노드에 입력값에 대한 변환 함수입니다.
노드의 입력 데이터는 활성화 함수를 거쳐 출력 함수로 변환 됩니다. (&amp;lt;그림 1&amp;gt; 참조)
 그림 1: 인공 신경망에서 노드의 입력 값을 변환하는 활성화 함수    인공 신경망에서 활성화 함수가 필요한 이유 여러개의 레이어로 구성되어 있고, 이전 레이어의 출력은 다음 레이어의 입력이 되는 신경망에서 모든 레이어가 Activation 함수로 선형 함수를 사용한다면, 이 신경망은 1개의 레이어로 표현 가능합니다.</description>
    </item>
    
  </channel>
</rss>