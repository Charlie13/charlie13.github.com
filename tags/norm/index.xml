<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Norm on taewan.kim</title>
    <link>/tags/norm/</link>
    <description>Recent content in Norm on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/norm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hidden Layer의 오차 계산</title>
      <link>/writing/norm/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/norm/</guid>
      <description>$$ \parallel x \parallel $$
Neural Network에서는 Forward Propagation 결과로 계산된 Output Layer 출력과 해당 입력 데이터 레이블의 차이를 계산하여 오차(손실, Error/Loss)를 계산합니다. 그리고 이 오차 최소화를 목표로 Hidden Layer들의 Weight(가중치)와 Bias(편향)를 업데이트합니다. 이렇게 딥러닝에서는 Neural Network에 데이터를 지속해서 흘려보내고, 오차를 계산한 후 Weight와 Bias를 수정하는 작업을 반복합니다.
Output Layer 출력과 레이블의 차이를 계산하고 은닉층의 Weight와 Bias를 업데이트하는 일련의 과정을 &amp;ldquo;역전파&amp;rdquo; 혹은 &amp;ldquo;Backpropagation&amp;ldquo;이라고 합니다. &amp;ldquo;Backpropagation&amp;ldquo;에서는 출력층의 오차로부터 은닉층의 노드별 오차를 계산하는 것이 핵심입니다.</description>
    </item>
    
    <item>
      <title>딥러닝의 주요 활성화 함수(Activation function)</title>
      <link>/writing/activation_function_derivative/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/activation_function_derivative/</guid>
      <description>Artificial Neural Network (ANN, 인공 신경망) 모델을 구성 할 때 가장 중요한 결정사항은 레이어 갯수와 은닉층(Hidden Layer)과 출력층(Output Layer)에서 사용할 활성화 함수를 결정하는 것입니다. 인공 신경망에서 활성화 함수(Activation function)는 노드에 입력값에 대한 변환 함수입니다.
노드의 입력 데이터는 활성화 함수를 거쳐 출력 함수로 변환 됩니다. (&amp;lt;그림 1&amp;gt; 참조)
 그림 1: 인공 신경망에서 노드의 입력 값을 변환하는 활성화 함수    인공 신경망에서 활성화 함수가 필요한 이유 여러개의 레이어로 구성되어 있고, 이전 레이어의 출력은 다음 레이어의 입력이 되는 신경망에서 모든 레이어가 Activation 함수로 선형 함수를 사용한다면, 이 신경망은 1개의 레이어로 표현 가능합니다.</description>
    </item>
    
  </channel>
</rss>