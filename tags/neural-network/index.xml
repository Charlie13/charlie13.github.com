<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Network on taewan.kim</title>
    <link>/tags/neural-network/</link>
    <description>Recent content in Neural Network on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/neural-network/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Coursera의 deeplearning.ai Course2 강좌 1주차 정리</title>
      <link>/writing/deeplearning_course2_week1/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/deeplearning_course2_week1/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하고 있는 Deep Learning 강좌 &amp;ldquo;5-course Specialization&amp;ldquo;의 두 번째 과정인 &amp;ldquo;Improving Deep Neural Network&amp;ldquo;의 1주차 강의를 정리합니다. 강좌의 세부 정보는 다음과 같습니다.
0. 강의 소개 본 문서에서 정리하는 강의 정보는 다음과 같습니다.
0.1 강좌 세부 정보  Mock: Coursera 운영: deeplearning.ai 과정명: 5-course Specialization 코스명: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizatio 주차: Week 1 주제: Practical aspect of Deep Learning  0.2 1주차 강의 학습 목표 1주차 강의 학습 목표는 다음과 같습니다.</description>
    </item>
    
    <item>
      <title>Hidden Layer의 오차 계산</title>
      <link>/post/error_in_hidden/</link>
      <pubDate>Fri, 08 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/error_in_hidden/</guid>
      <description>Neural Network에서는 Forward Propagation 결과로 계산된 Output Layer 출력과 해당 입력 데이터 레이블의 차이를 계산하여 오차(손실, Error/Loss)를 계산합니다. 그리고 이 오차 최소화를 목표로 Hidden Layer들의 Weight(가중치)와 Bias(편향)를 업데이트합니다. 이렇게 딥러닝에서는 Neural Network에 데이터를 지속해서 흘려보내고, 오차를 계산한 후 Weight와 Bias를 수정하는 작업을 반복합니다.
Output Layer 출력과 레이블의 차이를 계산하고 은닉층의 Weight와 Bias를 업데이트하는 일련의 과정을 &amp;ldquo;역전파&amp;rdquo; 혹은 &amp;ldquo;Backpropagation&amp;ldquo;이라고 합니다. &amp;ldquo;Backpropagation&amp;ldquo;에서는 출력층의 오차로부터 은닉층의 노드별 오차를 계산하는 것이 핵심입니다. 은닉층의 노드별 오차를 알아야 은닉층의 Weight와 Bias를 수정할 수 있습니다.</description>
    </item>
    
  </channel>
</rss>