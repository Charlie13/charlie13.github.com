<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>기계학습 on taewan.kim</title>
    <link>/tags/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/</link>
    <description>Recent content in 기계학습 on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/tags/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>$W_{ij}$와 $W_{ji}$ 차이? </title>
      <link>/post/wij_and_wji/</link>
      <pubDate>Fri, 22 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/wij_and_wji/</guid>
      <description>제가 처음에 딥러닝을 학습할 때 가장 혼란 스러웠던것은 은틱층의 입력 데이터와 가중치 W의 합을 표현하는 &amp;ldquo;Z(Weighted Sum)&amp;ldquo;을 표현하는 수식이 문서마다 다른 것 이었습니다.
  &amp;lt;식 1&amp;gt;. Z(Weighted Sum)을 표현하는 수식 $$ \begin{align} Z^{[l]} &amp;amp; = W^{[l]T}A^{[l-1]} &amp;amp; (1) \\
Z^{[l]} &amp;amp; = W^{[l]}A^{[l-1]} &amp;amp; (2) \end{align} $$   &amp;lt;식 1&amp;gt;의 (1)과 (2)는 다른 수식임에도 어떤 자료는 (1)과 같이 표현하고 어떤 자료는 (2)와 같이 표현합니다. &amp;lt;식 1&amp;gt; 수식의 표기법은 다음과 정리할 수 있습니다.</description>
    </item>
    
    <item>
      <title>딥러닝을 위한 Norm, 노름</title>
      <link>/post/norm/</link>
      <pubDate>Fri, 15 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/post/norm/</guid>
      <description>기계학습 자료에서 간혹 Norm과 관련된 수식이나 표기법을 나오면 당황스러울 때가 있습니다. 선형대수에 익숙하지 않다면 Norm이 이상하게 보일 수 있습니다. 본 문서에서는 인공신공망과 기계학습 일고리즘에서 사용되는 Norm을 이해하는 것을 목표로 최소한도의 Norm 개념을 정리합니다.
일반적으로 딥러닝에서 네트워크의 Overfitting(과적합) 문제를 해결하는 방법으로 다음과 같은 3가지 방법을 제시합니다.
 더 많은 데이터를 사용할 것 Cross Validation Regularization  더 이상 학습 데이터를 추가할 수 없거나 학습 데이터를 늘려도 과적합 문제가 해결되지 않을 때에는 3번 Regularization을 사용해야 합니다.</description>
    </item>
    
  </channel>
</rss>