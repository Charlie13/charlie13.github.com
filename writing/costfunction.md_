+++
date = "2017-11-25T21:28:14+09:00"
description = "Deep Learning에서 활성함수로 사용되는 tanh 함수의 미분 절차를 정리합니다. "
title = "Deep Learning 활성화 함수: Activation Function"
thumbnailInList = "https://taewanmerepo.github.io/2017/11/tanh/tanh.jpg"
thumbnailInPost = "https://taewanmerepo.github.io/2017/11/tanh/post.jpg"
tags = ["deep learning", "tanh", "활성함수", "미분", "sigmoid", "tahn", "ReLU", "Leak ReLU"]
categories = ["Machine Learning"]
author = "taewan.kim"
language = ""
jupyter = "false"
+++

딥러닝에서 뉴런에 유입된 입력 값을 출력값으로 변환하는 비선형 함수를 활성화 함수(Activation Function)이라고 합니다.
뉴런에 입력값과 출력값 변환은 다음
<font size="4" ><strong>
$$
\begin{align}
z & = w_1x_1 + w_2x_2 + .... + w_mx_m + b \\\\\
& = \sum_i^m w_ix_i \\\\\
& = W^tX+b \\\\\
a & = g(z) = \hat{y}
\end{align}
$$
</strong></font>

뉴럴네트워크의 개별 뉴런에 들어오는 입력신호의 총합을 출력신호로 변환하는 함수를 활성화함수(activation function)라고 합니다. 활성화함수 유무는 초창기 모델인 퍼셉트론(perceptron)과 뉴럴네트워크의 유일한 차이점이기도 하죠. 활성화함수는 대개 비선형함수(non-linear function)를 씁니다. 활성화함수로 왜 선형함수를 쓰면 안되는 걸까요? ‘밑바닥부터 시작하는 딥러닝’의 한 글귀를 인용해보겠습니다.

선형함수인 h(x)=cx
를 활성화함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))
가 됩니다. 이는 실은 y(x)=ax
와 똑같은 식입니다. a=c3
이라고만 하면 끝이죠. 즉, 은닉층이 없는 네트워크로 표현할 수 있습니다. 뉴럴네트워크에서 층을 쌓는 혜택을 얻고 싶다면 활성화함수로는 반드시 비선형 함수를 사용해야 합니다.
그러면 뉴럴네트워크 활성화함수 몇 가지 살펴보겠습니다.

## Sigmoid 함수

Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다.
Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다.
모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)[^1]에 자주 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.

[^1]: h = sigmoid(wx+b), L(h, y) = -ylog(h)-(1-y)log(1-h) 형태로 Logistic Classification의 비용함수에 사용됩니다.

딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다. Sigmoid 함수는 마이너스 값을 0에 가깝게 표현하기 때문에 입력값이 최종 계층에서 미치는 영향이 적어지는 __Vanishing gradient problem__ 이 발생합니다. 이 문제 때문에 일반적으로 sigmoid는 ReLU로 교체되어 사용됩니다. Sigmoid는 0부터 1.0까지 확률로 표시하기 때문에 Binary Classification의 출력 계층에서는 매우 유용합니다. Binary Classfication의 출력계층에 Sigmoid를 사용하면 결과를 확률로 확인할 수 있습니다.

Sigmoid 함수는 미분 결과가 간결하고 사용하기 쉽습니다. sigmoid는 미분 결과를 프로그래밍하기 쉽기에 인기가 더욱 높았습니다.

아래에서 Sigmoid 함수의 실체와 미분 유도 과정을 살펴보겠습니다.

### Sigmoid 함수

Sigmoid 함수는 다음과 같이 정의 됩니다.

<font size="4" ><strong>
$$sigmoid(x) = \frac{1}{1+e^{-x}}$$
</strong></font>


### Sigmoid 함수 미분

Sigmoid를 Logistic Classification의 가설로 사용하거나 딥러닝의 활성함수로 사용할 경우 경사 하강법(Gradient Descent Algorithm) 계산 혹은 역전파 계산 과정에서 Sigmoid 함수의 미분이 사용됩니다. 이런 과정의 이해를 돕는 목적으로 sigmoid의 미분 과정을 정리합니다.

sigmoid 함수 미분 절차는 다음과 같습니다.

><font size="4" >
$$
\begin{align}
\frac{d}{dx}sigmoid(x) & = \frac{d}{dx}{(1+e^{-x})^{-1}} \\\\\
& = (-1)\frac{1}{(1+e^{-x})^{2}}\frac{d}{dx}(1+e^{-x}) \\\\\
& = (-1)\frac{1}{(1+e^{-x})^{2}}(0+e^{-x})\frac{d}{dx}(-x) \\\\\
& = (-1)\frac{1}{(1+e^{-x})^{2}}e^{-x}(-1)  \\\\\
& = \frac{e^{-x}}{(1+e^{-x})^{2}}  \\\\\
& = \frac{1+e^{-x}-1}{(1+e^{-x})^{2}}  \\\\\
& = \frac{(1+e^{-x})}{(1+e^{-x})^{2}}-\frac{1}{(1+e^{-x})^{2}}  \\\\\
& = \frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^{2}}  \\\\\
& = \frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}}) \\\\\
& = sigmoid(x)(1-sigmoid(x))
\end{align}
$$
</font>

sigmoid의 미분 결과(도함수)는 다음과 같습니다.
앞에서 언급한 것처럼 sigmoid의 미분 결과는 매우 간결한 것 을 확인할 수 있습니다.

><font size="4" ><strong>
$$
\frac{d}{dx}sigmoid(x) = sigmoid(x)(1-sigmoid(x))
$$
</strong></font>

다음은 sigmoid 미분결과(도함수)로 그래프를 그린 결과입니다.

![](https://taewanmerepo.github.io/2017/09/sigmoid/differential_sigmoid.jpg)

그래프의 미분계수를 보면 최댓값은 0.25입니다. deep learning에서 학습을 위하여 역전파(Backpropagation)를 계산하는 과정에서 activation function의 미분 값을 곱하는 과정이 포함됩니다. sigmoid를 활성 함수로 사용하는 경우 은닉층의 깊이가 깊다면 오차율 계산이 어렵다는 문제가 발생합니다. 이것이 딥러닝에 sigmoid를 사용할 때  "__vanishing gradient problem__"이 발행하는 이유입니다.

### sigmoid 함수 구현

sigmoid 코드 구현은 매우 간단합니다. sigmoid의 구현 코드를 정리합니다.

파이썬으로 구현한 sigmoid 함수는 다음과 같습니다.

```
import math

def sigmoid(x, deff=False):
  if deff:
    return sigmoid(x)*(1-sigmoid(x))
  else:
    return 1 / (1 + math.exp(-x))
```

스칼라로 구현한 sigmoid 함수는 다음과 같습니다.

```
import scala.math

def sigmoid(x: Double): Double = {
  return 1.0 / (1.0 + math.pow(math.E, -x))
}

```

## Hyperbolic Tangent (tanh)

### tanh 함수 정의

tanh 함수는 다음과 같이 정의 됩니다

> $$
\begin{align}
tanh(x) & = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\\
& =  \frac{e^x - e^{-x}}{e^x + e^{-x}}(\frac{e^x}{e^x})  \\\\\
& =  \frac{e^{2x} - 1}{e^{2x} + 1}
\end{align}
$$

## Hyperbolic Function 미분 유도

> $$
\begin{align}
\frac{\partial J(W,b) }{\partial W} & = \\\\\
& = (-1)\frac{1}{(1+e^{-x})^{2}}(0+e^{-x})\frac{d}{dx}(-x) \\\\\
& = (-1)\frac{1}{(1+e^{-x})^{2}}e^{-x}(-1)  \\\\\
& = \frac{e^{-x}}{(1+e^{-x})^{2}}  \\\\\
& = \frac{1+e^{-x}-1}{(1+e^{-x})^{2}}  \\\\\
& = \frac{(1+e^{-x})}{(1+e^{-x})^{2}}-\frac{1}{(1+e^{-x})^{2}}  \\\\\
& = \frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^{2}}  \\\\\
& = \frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}}) \\\\\
& = sigmoid(x)(1-sigmoid(x))
\end{align}
$$

## ReLU
## Leak ReLU


## 참고자료
- https://en.wikipedia.org/wiki/Sigmoid_function
- [CHAPTER 5.Why are deep neural networks hard to train?](http://neuralnetworksanddeeplearning.com/chap5.html)
