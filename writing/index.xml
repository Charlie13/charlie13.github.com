<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Writings on taewan.kim</title>
    <link>/writing/</link>
    <description>Recent content in Writings on taewan.kim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Dec 2017 21:28:14 +0900</lastBuildDate>
    
	<atom:link href="/writing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Coursera의 deeplearning.ai Course2 강좌 1주차 정리</title>
      <link>/writing/deeplearning_course2_week1/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/deeplearning_course2_week1/</guid>
      <description>Coursera에서 deeplearning.ai가 진행하고 있는 Deep Learning 강좌 &amp;ldquo;5-course Specialization&amp;ldquo;의 두 번째 과정인 &amp;ldquo;Improving Deep Neural Network&amp;ldquo;의 1주차 강의를 정리합니다. 강좌의 세부 정보는 다음과 같습니다.
0. 강의 소개 본 문서에서 정리하는 강의 정보는 다음과 같습니다.
0.1 강좌 세부 정보  Mock: Coursera 운영: deeplearning.ai 과정명: 5-course Specialization 코스명: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizatio 주차: Week 1 주제: Practical aspect of Deep Learning  0.2 1주차 강의 학습 목표 1주차 강의 학습 목표는 다음과 같습니다.</description>
    </item>
    
    <item>
      <title>Hidden Layer의 오차 계산</title>
      <link>/writing/norm/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/norm/</guid>
      <description>$$ \parallel x \parallel $$
Neural Network에서는 Forward Propagation 결과로 계산된 Output Layer 출력과 해당 입력 데이터 레이블의 차이를 계산하여 오차(손실, Error/Loss)를 계산합니다. 그리고 이 오차 최소화를 목표로 Hidden Layer들의 Weight(가중치)와 Bias(편향)를 업데이트합니다. 이렇게 딥러닝에서는 Neural Network에 데이터를 지속해서 흘려보내고, 오차를 계산한 후 Weight와 Bias를 수정하는 작업을 반복합니다.
Output Layer 출력과 레이블의 차이를 계산하고 은닉층의 Weight와 Bias를 업데이트하는 일련의 과정을 &amp;ldquo;역전파&amp;rdquo; 혹은 &amp;ldquo;Backpropagation&amp;ldquo;이라고 합니다. &amp;ldquo;Backpropagation&amp;ldquo;에서는 출력층의 오차로부터 은닉층의 노드별 오차를 계산하는 것이 핵심입니다.</description>
    </item>
    
    <item>
      <title>딥러닝의 주요 활성화 함수(Activation function)</title>
      <link>/writing/activation_function_derivative/</link>
      <pubDate>Mon, 11 Dec 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/activation_function_derivative/</guid>
      <description>Artificial Neural Network (ANN, 인공 신경망) 모델을 구성 할 때 가장 중요한 결정사항은 레이어 갯수와 은닉층(Hidden Layer)과 출력층(Output Layer)에서 사용할 활성화 함수를 결정하는 것입니다. 인공 신경망에서 활성화 함수(Activation function)는 노드에 입력값에 대한 변환 함수입니다.
노드의 입력 데이터는 활성화 함수를 거쳐 출력 함수로 변환 됩니다. (&amp;lt;그림 1&amp;gt; 참조)
 그림 1: 인공 신경망에서 노드의 입력 값을 변환하는 활성화 함수    인공 신경망에서 활성화 함수가 필요한 이유 여러개의 레이어로 구성되어 있고, 이전 레이어의 출력은 다음 레이어의 입력이 되는 신경망에서 모든 레이어가 Activation 함수로 선형 함수를 사용한다면, 이 신경망은 1개의 레이어로 표현 가능합니다.</description>
    </item>
    
    <item>
      <title>Logistic regression의 Cost Function과 미분</title>
      <link>/writing/logistic_regression_diff/</link>
      <pubDate>Fri, 30 Jun 2017 21:28:14 +0900</pubDate>
      
      <guid>/writing/logistic_regression_diff/</guid>
      <description>Sigmoid 함수는 S자와 유사한 완만한 시그모이드 커브 형태를 보이는 함수입니다. Sigmoid는 대표적인 Logistic 함수입니다. Sigmoid 함수는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다. 모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)1에 많이 사용됩니다. sigmoid의 반환 값은 확률형태이기 때문에 결과를 확률로 해석할 때 유용합니다.
딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성 함수로도 이용됩니다.</description>
    </item>
    
  </channel>
</rss>