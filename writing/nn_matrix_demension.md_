+++
date = "2017-11-29T21:28:14+09:00"
description = "Neural Network에서 사용되는 표기법을 정리합니다."
title = "신경망 Weight&Bias Shape의 일반화"
thumbnailInList = "https://taewanmerepo.github.io/2017/11/nn-notation/post.jpg"
thumbnailInPost = "https://taewanmerepo.github.io/2017/11/nn-notation/nn.jpg"
tags = ["deep learning", "표기법", "notation", "Coursera", "deeplearning.ai"]
categories = ["Machine Learning"]
author = "taewan.kim"
language = ""
jupyter = "false"
+++

Coursera에서 deeplearning.ai가 진행하는 __Neural Networks and Deep Learning__ 강의(4주차: Getting your matrix dimensions)에서 다룬 "__Neural Network의 Weight와 Bias Shape 계산 일반화__"를 정리합니다.


### layer 별 연산

각 레이어에는 Column Vector 형태로 데이터가 입력됩니다. Column Vector란 m개의 속성을 갖을 데이터 인스턴스 1개의 입력 데이터의 shape은 (m, 1)인 벡터 입니다. 입력 데이터가 m개의 속성을 갖는 n개의 데이터 인스턴스로 구성될 경우 입력 데이터의 shape은  (m, n)입니다.

- Layer 1 연산 & parameter shape

Input Feature로 단일 데이터(Single Instance)로 데이터가 유일될 경우 Layer 1에서는 다음과 같은 연산이 이루어 집니다.

>$$
\begin{align}
Z^{\[1\]} & = W^{\[1\]} + b^{\[1\]} \\\\\
A^{[^1]} & = g^{[1]}(Z^{\[1\]})
\end{align}
$$

|컴포넌트|Shape|
|---|---|
|$$X$$|(2, 1)|
|$$Layer 2$$|(5, 1)|



### layer 1 연산



## Neural Network의 레이어 표기법


Neural Netowk의 레이어 표기법은 Input Feature를 "Layer 0"로 표시합니다. 첫 번째 Hidden Layer부터 "Layer 1", "Layer 2"로 표기합니다. Hidden Layer와 Output Layer의 개수를 "L"문자로 표기합니다.

|표기|구분|설명|
|---|---|---|
|L=5|#layers|Neural Newwork 레이어 수|
|layer 0|layer 지정자|Input Feature 레이어|
|layer 1|layer 지정자|첫번째 은닉층|
|layer 2|layer 지정자|두번째 은닉층|
|layer 3|layer 지정자|세번째 은닉층|
|layer 4|layer 지정자|네번째 은닉층|
|layer 5|layer 지정자|출력층|

각 레이어의 node 수는 다음과 같은 형식으로 표현합니다. 아래 형식에서 __ㅣ__ (소문자 L)은 레이어 번호입니다.

$$
n_x: Number of Input features \\\\\
n^{\[l\]}: Number of Layer's node
$$

각 레이어의 노드 수는 다음과 같이 표현할 수 있습니다.

|표기|노드 수|설명|
|---|---|---|
|$$n_x=n^{\[0\]}$$|3| input feature 수  |
|$$n^{\[1\]}$$| 5 | layer 1의 노드 수  |
|$$n^{\[2\]}$$| 5 | layer 2의 노드 수  |
|$$n^{\[3\]}$$| 5 | layer 3의 노드 수  |
|$$n^{\[4\]}$$| 3 | layer 4의 노드 수  |
|$$n^{\[5\]}=n^{\[L\]}$$| 1 | layer 5의 output layer의 노드 수  |

## Neural Network의 표기법

각 레이어에 대한 z, activation function, output, Weights 및 Bias에 대한 표기법을 다음과 같이 정리할 수 있습니다.

|표기 형식|유형|설명|
|---|---|---|
|$$z^{\[l\]}$$| Sum-Product(z) | 지정된 레이어의 sum-product의 컬럼 벡터   |
|$$g^{\[l\]}$$| activation function | 지정된 레이어가 사용하는 activation function|
|$$a^{\[l\]}$$| output 값| sum-product에 activation이 적용된 출력 컬럼 벡터  |
|$$W^{\[l\]}$$| 가중치(weights) | 지정된 레이어의 Weight |
|$$b^{\[l\]}$$| 편향(bias) | 지정된 레이어의 bias  |

### 표기법 사용 예제

위 표기법을 사용하여 그림 1은 다음과 같이 정리할 수 있습니다.

|항목| 값|설명|
|---|---|---|
|$$L$$| 5 |네트워크 레이어 5개|
|$$n_x$$| 3 |input feature 3개|

|항목|layer 1|layer 2|layer 3|layer 4|layer 5|
|---|---|---|---|---|---|
|레이어별 노드 수| $$n^{\[1\]}=5$$ | $$n^{\[2\]}=5$$ | $$n^{\[3\]}=5$$ | $$n^{\[4\]}=3$$ | $$n^{\[5\]}=1$$ |
|레이어별 활성 함수| relu | relu | relu | relu | sigmoid |
|레이어별 Weight의 shape| $$W^{\[1\]}=(n^{\[1\]},n_x)$$ | $$W^{\[2\]}=(n^{\[2\]},n^{\[1\]})$$ | $$W^{\[3\]}=(n^{\[3\]},n^{\[2\]})$$ | $$W^{\[4\]}=(n^{\[4\]},n^{\[3\]})$$ | $$W^{\[5\]}=(n^{\[5\]},n^{\[4\]})$$ |
|레이어별 bias의 shape| $$w^{\[1\]}=(n^{\[1\]},1)$$ | $$w^{\[2\]}=(n^{\[2\]},1)$$ | $$w^{\[3\]}=(n^{\[3\]},1)$$ | $$w^{\[4\]}=(n^{\[4\]},1)$$ | $$w^{\[5\]}=(n^{\[5\]},1)$$ |

<그림 1>의 Neural Network의 표기법을 그림으로 정리하면 <그림2>와 같습니다.

{{< img src="https://taewanmerepo.github.io/2017/11/nn-notation/nn_notation.jpg"
title="그림 2"
caption="Neural Network 표기법 적용" >}}


## Neural Network의 표기법 확장

위에서 설명한 표기법은 기본적으로 레이어 단위입니다. 레이어 하위인 단위 노드와 여러 데이터 입력에 대한 입력 데이터 단위로 표기법을 확장할 수 있습니다.

<font size="4" ><strong>
$$
P^{\[l\]\(nth\)}_{nid}
$$
</strong></font>

- P: 표기 항목
- l: 레이어 번호
- nid: 레이어의 node 번호
- nth: n 번째 입력 데이터

nid와 nth를 이용하여 특정 입력 데이터와 특정 노드 지정이 가능합니다.

### 표기법 확장 예제

확장된 표기법은 다음과 같이 사용할 수 있습니다.

|예제| 설명|
|---|---|
|$$a^{\[1\]}$$| layer 1의 출력 벡터 |
|$$a^{\[1\]}_1$$| layer 1의 첫번째 노드의 출력 값 |
|$$a^{\[1\]\(3\)}$$| 3번째 입력 데이터에 대한 layer 1의 출력 벡터 |
|$$a^{\[1\]\(3\)}_1$$| 3번째 입력 데이터에 대한 layer 1의 첫 번째 노드 출력 값 |

## 참고자료
- [Coursera: Neural Networks and Deep Learning by deeplearning.ai, 4Week: Deep Neural Networks - Getting your matrix dimensions right](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right)
