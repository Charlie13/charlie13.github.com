+++
date = "2014-12-01T21:28:14+09:00"
description = "Neural Networkì—ì„œ ì‚¬ìš©ë˜ëŠ” í‘œê¸°ë²•ì„ ì •ë¦¬í•©ë‹ˆë‹¤."
title = "Neural Networkì˜ Backward Propagationì˜ ì¼ë°˜í™”"
thumbnailInList = "https://taewanmerepo.github.io/2017/12/backpropatation/list2.jpg"
thumbnailInPost = "https://taewanmerepo.github.io/2017/12/backpropatation/post.jpg"
tags = ["deep learning", "backpropagation", "Coursera", "deeplearning.ai"]
categories = ["Machine Learning"]
author = "taewan.kim"
language = ""  
jupyter = "false"
+++

$a^1=b$

Courseraì—ì„œ deeplearning.aiê°€ ì§„í–‰í•˜ëŠ” __Neural Networks and Deep Learning__ ê°•ì˜(4ì£¼ì°¨: Forward and Backward Propagation)ì—ì„œ ë‹¤ë£¬ "__Neural Networkì˜ Forward Propagationê³¼ Backward Propagation ì¼ë°˜í™”__"ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

Neural Networkì—ì„œëŠ” Forward Propagationì„ ì´ìš©í•˜ì—¬ ì…ë ¥í•œ ë°ì´í„°ì˜ ì¶”ì •ê²°ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
ì§€ë„í•™ìŠµì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ì˜ ì¶”ì •ê²°ê³¼ì™€ í•´ë‹¹ ë°ì´í„°ì˜ ì‹¤ì œ ì •ë‹µì¸ ë ˆì´ë¸”(Label)ê³¼ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë•Œ "__Error Function/Loss Function__"ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì •ê²°ê³¼ì™€ ë ˆì´ë¸”ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. "__Error Function/Loss Function__"ì„ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ê³¼ Weight, Biasë¡œ ë¯¸ë¶„í•˜ì—¬ ê° ë ˆì´ì–´ì˜ Weight ì™€ Biasê°€ ì˜¤ì°¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ê° ë ˆì´ì–´ì˜ Weightì™€ Biasì— ë°˜ì˜(ì—…ë°ì´íŠ¸) í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì¶”ì •ê²°ê³¼ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ê³ ì ê° ë ˆì´ì–´ì˜ Weightì™€ Biasë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°˜ë³µì ì¸ ì‘ì—…ì„ "__í•™ìŠµ(Training)__"ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì…ë ¥ ë°ì´í„°ì˜ ì¶”ì •ê²°ê³¼ì™€ ë ˆì´ë¸” ì˜¤ì°¨ì— ê° ë ˆì´ì–´ì˜ Weightì™€ Biasê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³„ì‚°í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •ì„ "__Backward Propagation__"ì´ë¼ê³  í•©ë‹ˆë‹¤.

Error Functionì— ëŒ€í•œ ë¯¸ë¶„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ë ˆì´ì–´ì˜ Weightì™€ Biasê°€ ì˜¤ì°¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³„ì‚°í•˜ê³  ê¸°ì¡´ Weightì™€ Bias ë°˜ì˜í•˜ëŠ” í•™ìŠµì„ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
& W^{\[l\]} := W^{\[l\]} - \alpha \frac{\partial }{\partial W} L(a, y) \\\\\
& b^{\[l\]} := b^{\[l\]} - \alpha \frac{\partial }{\partial b} L(a, y)
\end{align}
$$

- ğœ¶ : Learning rate
- a : ë…¸ë“œì˜ Output
- dL/dw[l] : í•´ë‹¹ ë ˆì´ì–´ì—ì„œ weight ë¯¸ë¶„ ê²°ê³¼  
- dL/db[l] : í•´ë‹¹ ë ˆì´ì–´ì—ì„œ bias ë¯¸ë¶„ ê²°ê³¼  

Forward Propagationê³¼ Backward Propagation ê³„ì‚°ì„ ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ Neural Networkë¥¼ ì˜ˆì œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

{{< img src="https://taewanmerepo.github.io/2017/12/backpropatation/nn.jpg"
title="ê·¸ë¦¼ 1"
caption="ì˜ˆì œ Neural Network: L=4" >}}

<ê·¸ë¦¼ 1> Neural Networkì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Network Layer(L): 4

|ì†ì„±|Layer 0|Layer 1|Layer 2|Layer 3|Labyer 4|
|---|---|---|---|---|---|
|$$Input Data Shape$$|(3,1)|(5,1)|(3,1)|(2,1)|(1,1)|
|$$W^{\[l\]} Shape$$|X|(5,3)|(3,5)|(2,3)|(1,2)|
|$$b^{\[l\]} Shape$$|X|(5,1)|(3,1)|(2,1)|(1,1)|
|$$Activation function$$|X|relu|relu|relu|sigmoid|
|$$n^{\[l\]}$$|3|5|3|2|1|


## Forward Propagation

Forward Propagationë¥¼ í‘œí˜„í•¨ì— ìˆì–´ì„œ ì‚¬ìš©í•˜ëŠ” ë¬¸ìëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- a[l]: ê° ë ˆì´ì–´ì˜ ì¶œë ¥ê°’
- g[l]: ê° ë ˆì´ì–´ì˜ í™œì„±í™” í•¨ìˆ˜(Activation Function)
- W[l]: í•´ë‹¹ ë ˆì´ì–´ì˜ Weight ë²¡í„°
- b[l]: í•´ë‹¹ ë ˆì´ì–´ì˜ bias ì»¬ëŸ¼ ë²¡í„°

ê° ë ˆì´ì–´ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Layer 1ì˜ Forward Propagation

Layer 1ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
Z^{\[1\]} & = W^{\[1\]}X + b^{\[1\]} \\\\\
A^{[^1]} & = g^{[1]}(Z^{\[1\]}) \\\\\
\end{align}
$$

Input Featureì¸ xë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
X = a^{\[0\]}
\end{align}
$$

Xë¥¼ ì¹˜í™˜í•˜ì—¬ Layer 1ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
Z^{\[1\]} & = W^{\[1\]}a^{\[0\]} + b^{\[1\]} \\\\\
A^{[^1]} & = g^{[1]}(Z^{\[1\]})
\end{align}
$$

### Layer 2ì˜ Forward Propagation

Layer 2ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
Z^{\[2\]} & = W^{\[2\]}a^{\[1\]} + b^{\[2\]} \\\\\
A^{\[2\]} & = g^{\[2\]}(Z^{\[2\]})
\end{align}
$$

### Layer 3ì˜ Forward Propagation

Layer 3ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
Z^{\[3\]} & = W^{\[3\]}a^{\[2\]} + b^{\[3\]} \\\\\
A^{\[3\]} & = g^{\[3\]}(Z^{\[3\]})
\end{align}
$$

### Layer 4ì˜ Forward Propagation

Layer 4ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
Z^{\[4\]} & = W^{\[4\]}a^{\[3\]} + b^{\[4\]} \\\\\
A^{[^4]} & = g^{\[4\]}(Z^{\[4\]})
\end{align}
$$

## Forward Propagation ì¼ë°˜í™”

Layer 1ë¶€í„° Layer 4ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë°˜í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
a^{\[0\]} & = Input Feature X \\\\\
Z^{\[l\]} & = W^{\[l\]}a^{\[l-1\]} + b^{\[l\]} \\\\\
A^{\[l\]} & = g^{[l]}(Z^{\[l\]})
\end{align}
$$

## Loss Function

ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ activation functionì´ sigmoidì´ê¸° ë•Œë¬¸ì— ì¶œë ¥ê°’(a[4])ì€ í™•ë¥ ì˜ í˜•íƒœ(0<a[4]<1)ì…ë‹ˆë‹¤.  
ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ Forward Propagationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>$$
\begin{align}
L & = 4 \\\\\
Z^{\[4\]} & = W^{\[4\]}a^{\[3\]} + b^{\[4\]} \\\\\
A^{[^4]} & = g^{\[4\]}(Z^{\[4\]}) \\\\\
\hat{y} & =  A^{\[L\]} = g^{\[L\]}(Z^{\[L\]}) \\\\\
\end{align}
$$

ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ í™œì„±í™” í•¨ìˆ˜ê°€ sigmoidë¡œ <ê·¸ë¦¼ 1> Neural Networkì€ Binary Classificationë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë„¤íŠ¸ì–´í¬ì˜ Loss Functionìœ¼ë¡œëŠ” Cross Entropyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Cross EntropyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ë©ë‹ˆë‹¤.

>$$
\begin{align}
L(\hat{y}, y) & =  -(ylog(\hat{y}) + (1-y)log(1-\hat{y}))
\end{align}
$$

## Backward Propagation

Loss Functionì„ Neural Networkì˜ ì¶œë ¥ê°’(a[L])ìœ¼ë¡œ ë¯¸ë¶„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ a[4], a[L], y_hatì€ ëª¨ë‘ ê°™ì€ í‘œí˜„ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Neural Networkì˜ ìµœì¢… ì¶œë ¥ ê°’ì„ y_hatìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. a[L]ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ í™œì„±í•¨ìˆ˜ ì¶œë ¥ê°’ì…ë‹ˆë‹¤. ë³¸ë¬¸ì—ì„œ ì˜ˆì œë¡œ ì‚¬ìš©í•˜ëŠ” Neural Networkì€ 4ê°œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ìˆê¸° ë•Œë¬¸ì— a[4]ì™€ a[L], y_hatì€ ëª¨ë‘ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

>$$
\begin{align}
\frac{\partial L(a^{\[L\]} , y)}{\partial a^{\[L\]} } & = \frac{-y}{a^{\[L\]}} - \frac{1-y}{1-a^{\[L\]} }(1-a^{\[L\]})' \\\\\
& = \frac{-y}{a^{\[L\]} } - \frac{1-y}{1-a^{\[L\]}}(-1) \\\\\
& = \frac{-y}{a^{\[L\]} } + \frac{1-y}{1-a^{\[L\]} } \\\\\
da^{\[L\]} & = \frac{-y}{a^{\[L\]} } + \frac{1-y}{1-a^{\[L\]} }
\end{align}
$$

{{< img src="https://taewanmerepo.github.io/2017/12/backpropatation/backprop01.jpg"
title="ê·¸ë¦¼ 2"
caption="Backward Propagationì˜ Input: Layer 4ì˜ ì…ë ¥" >}}

Backward Propagationì—ì„œ ì²« ë²ˆì§¸ Layer 4ì˜ ì…ë ¥ì€ da[4]ì…ë‹ˆë‹¤. da[4]ì˜ ì…ë ¥ì„ í¸ë¯¸ë¶„í•˜ì—¬ dw[4]ì™€ db[4]ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. dw[4]ì™€ db[4]ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ì‚¬ì „ ì¤€ë¹„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

>$$
\begin{align}
a^{[^l]} & = g^{\[l\]}(Z^{\[l\]}) \\\\\
z^{\[l\]} & = W^{\[l\]}a^{\[l-1\]} + b^{\[l\]} \\\\\
\frac{\partial a^{[^l]}}{\partial z^{\[l\]}} & = \frac{\partial}{\partial z}g^{\[l\]}(z^{\[l\]}) = g^{\[l\]'}(z^{\[l\]}) \\\\\
\frac{\partial z^{\[^l\]}}{\partial W^{\[l\]}} & = \frac{\partial}{\partial W^{\[l\]}}(W^{\[l\]}a^{\[l-1\]} + b^{\[l\]}) = a^{\[l-1\]} \\\\\
\frac{\partial z^{\[^l\]}}{\partial b^{\[l\]}} & = \frac{\partial}{\partial b^{\[l\]}}(W^{\[l\]}a^{\[l-1\]} + b^{\[l\]}) = 1
\end{align}
$$

dw[4]ì™€ db[4]ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

>$$
\begin{align}
\frac{\partial L(a^{\[4\]}, y)}{\partial z^{\[4\]}} & =  \frac{\partial L(a^{\[4\]}, y)}{\partial a^{\[4\]}}\frac{\partial a^{\[4\]}}{\partial z^{\[4\]}} \\\\\
& = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) \\\\\
dz^{\[4\]} & = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) \\\\\
\frac{\partial L(a^{\[4\]}, y)}{\partial W^{\[4\]}} & = \frac{\partial L(a^{\[4\]}, y)}{\partial a^{\[4\]}}\frac{\partial a^{\[4\]}}{\partial z^{\[4\]}}\frac{\partial z^{\[4\]}}{\partial W^{\[4\]}} \\\\\
& = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) * a^{\[3\]} \\\\\
& = dz^{\[4\]} * a^{\[3\]} \\\\\
\frac{\partial L(a^{\[4\]}, y)}{\partial b^{\[4\]}} & = \frac{\partial L(a^{\[4\]}, y)}{\partial a^{\[4\]}}\frac{\partial a^{\[4\]}}{\partial z^{\[4\]}}\frac{\partial z^{\[4\]}}{\partial b^{\[4\]}} \\\\\
& = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) \\\\\
& = dz^{\[4\]}
\end{align}
$$




#### ã…‡ã…‡ã…‡

>$$
\begin{align}
& dZ^{\[L\]} = A^{\[L\]} - Y \\\\\
& dW^{\[L\]} = \frac{1}{m}dZ^{\[L\]}A^{\[L\]^T} \\\\\
& db^{\[L\]} = \frac{1}{m}np.sum(dZ^{\[L\]}, axis=1, keepdims=True) \\\\\
& dZ^{\[L-1\]} = dW^{\[L\]^T}dZ^{\[L\]}g'^{\[L\]}(Z^{\[L\]}) \\\\\
& .... \\\\\
& dZ^{\[1\]} = dW^{\[L\]^T}dZ^{\[2\]}g'^{\[1\]}(Z^{\[1\]}) \\\\\
& dW^{\[1\]} = \frac{1}{m}dZ^{\[1\]}A^{\[1\]^T} \\\\\
& db^{\[1\]} = \frac{1}{m}np.sum(dZ^{\[1\]}, axis=1, keepdims=True)  
\end{align}
$$

## ì°¸ê³ ìë£Œ
- [Coursera: Neural Networks and Deep Learning by deeplearning.ai, 4Week: Deep Neural Networks - Getting your matrix dimensions right](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right)
