+++
date = "2014-12-01T21:28:14+09:00"
description = "Neural Network에서 사용되는 표기법을 정리합니다."
title = "Neural Network의 Backward Propagation의 일반화"
thumbnailInList = "https://taewanmerepo.github.io/2017/12/backpropatation/list2.jpg"
thumbnailInPost = "https://taewanmerepo.github.io/2017/12/backpropatation/post.jpg"
tags = ["deep learning", "backpropagation", "Coursera", "deeplearning.ai"]
categories = ["Machine Learning"]
author = "taewan.kim"
language = ""  
jupyter = "false"
+++

$a^1=b$

Coursera에서 deeplearning.ai가 진행하는 __Neural Networks and Deep Learning__ 강의(4주차: Forward and Backward Propagation)에서 다룬 "__Neural Network의 Forward Propagation과 Backward Propagation 일반화__"를 정리합니다.

Neural Network에서는 Forward Propagation을 이용하여 입력한 데이터의 추정결과를 계산합니다.
지도학습에서는 입력 데이터의 추정결과와 해당 데이터의 실제 정답인 레이블(Label)과의 오차를 계산합니다. 이때 "__Error Function/Loss Function__"을 사용하여 추정결과와 레이블의 오차를 계산합니다. "__Error Function/Loss Function__"을 각 레이어의 출력값과 Weight, Bias로 미분하여 각 레이어의 Weight 와 Bias가 오차에 미치는 영향도를 계산합니다. 이 결과를 각 레이어의 Weight와 Bias에 반영(업데이트) 합니다. 이렇게 입력 데이터에 대한 추정결과의 오차를 줄이고자 각 레이어의 Weight와 Bias를 업데이트하는 반복적인 작업을 "__학습(Training)__"이라고 합니다. 이렇게 입력 데이터의 추정결과와 레이블 오차에 각 레이어의 Weight와 Bias가 미치는 영향을 계산하는 일련의 과정을 "__Backward Propagation__"이라고 합니다.

Error Function에 대한 미분 결과를 바탕으로 각 레이어의 Weight와 Bias가 오차에 미치는 영향을 계산하고 기존 Weight와 Bias 반영하는 학습을 수학적으로 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
& W^{\[l\]} := W^{\[l\]} - \alpha \frac{\partial }{\partial W} L(a, y) \\\\\
& b^{\[l\]} := b^{\[l\]} - \alpha \frac{\partial }{\partial b} L(a, y)
\end{align}
$$

- 𝜶 : Learning rate
- a : 노드의 Output
- dL/dw[l] : 해당 레이어에서 weight 미분 결과  
- dL/db[l] : 해당 레이어에서 bias 미분 결과  

Forward Propagation과 Backward Propagation 계산을 일반화하기 위해서 다음과 같은 Neural Network를 예제로 사용합니다.

{{< img src="https://taewanmerepo.github.io/2017/12/backpropatation/nn.jpg"
title="그림 1"
caption="예제 Neural Network: L=4" >}}

<그림 1> Neural Network은 다음과 같이 정리할 수 있습니다.

- Network Layer(L): 4

|속성|Layer 0|Layer 1|Layer 2|Layer 3|Labyer 4|
|---|---|---|---|---|---|
|$$Input Data Shape$$|(3,1)|(5,1)|(3,1)|(2,1)|(1,1)|
|$$W^{\[l\]} Shape$$|X|(5,3)|(3,5)|(2,3)|(1,2)|
|$$b^{\[l\]} Shape$$|X|(5,1)|(3,1)|(2,1)|(1,1)|
|$$Activation function$$|X|relu|relu|relu|sigmoid|
|$$n^{\[l\]}$$|3|5|3|2|1|


## Forward Propagation

Forward Propagation를 표현함에 있어서 사용하는 문자는 다음과 같습니다.

- a[l]: 각 레이어의 출력값
- g[l]: 각 레이어의 활성화 함수(Activation Function)
- W[l]: 해당 레이어의 Weight 벡터
- b[l]: 해당 레이어의 bias 컬럼 벡터

각 레이어의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

### Layer 1의 Forward Propagation

Layer 1의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
Z^{\[1\]} & = W^{\[1\]}X + b^{\[1\]} \\\\\
A^{[^1]} & = g^{[1]}(Z^{\[1\]}) \\\\\
\end{align}
$$

Input Feature인 x를 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
X = a^{\[0\]}
\end{align}
$$

X를 치환하여 Layer 1의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
Z^{\[1\]} & = W^{\[1\]}a^{\[0\]} + b^{\[1\]} \\\\\
A^{[^1]} & = g^{[1]}(Z^{\[1\]})
\end{align}
$$

### Layer 2의 Forward Propagation

Layer 2의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
Z^{\[2\]} & = W^{\[2\]}a^{\[1\]} + b^{\[2\]} \\\\\
A^{\[2\]} & = g^{\[2\]}(Z^{\[2\]})
\end{align}
$$

### Layer 3의 Forward Propagation

Layer 3의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
Z^{\[3\]} & = W^{\[3\]}a^{\[2\]} + b^{\[3\]} \\\\\
A^{\[3\]} & = g^{\[3\]}(Z^{\[3\]})
\end{align}
$$

### Layer 4의 Forward Propagation

Layer 4의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
Z^{\[4\]} & = W^{\[4\]}a^{\[3\]} + b^{\[4\]} \\\\\
A^{[^4]} & = g^{\[4\]}(Z^{\[4\]})
\end{align}
$$

## Forward Propagation 일반화

Layer 1부터 Layer 4의 Forward Propagation은 다음과 같이 일반화 할 수 있습니다.

>$$
\begin{align}
a^{\[0\]} & = Input Feature X \\\\\
Z^{\[l\]} & = W^{\[l\]}a^{\[l-1\]} + b^{\[l\]} \\\\\
A^{\[l\]} & = g^{[l]}(Z^{\[l\]})
\end{align}
$$

## Loss Function

마지막 레이어의 activation function이 sigmoid이기 때문에 출력값(a[4])은 확률의 형태(0<a[4]<1)입니다.  
마지막 레이어의 Forward Propagation은 다음과 같이 표현할 수 있습니다.

>$$
\begin{align}
L & = 4 \\\\\
Z^{\[4\]} & = W^{\[4\]}a^{\[3\]} + b^{\[4\]} \\\\\
A^{[^4]} & = g^{\[4\]}(Z^{\[4\]}) \\\\\
\hat{y} & =  A^{\[L\]} = g^{\[L\]}(Z^{\[L\]}) \\\\\
\end{align}
$$

마지막 레이어의 활성화 함수가 sigmoid로 <그림 1> Neural Network은 Binary Classification로 예상됩니다. 따라서 이 네트어크의 Loss Function으로는 Cross Entropy를 사용합니다. Cross Entropy는 다음과 같이 정의 됩니다.

>$$
\begin{align}
L(\hat{y}, y) & =  -(ylog(\hat{y}) + (1-y)log(1-\hat{y}))
\end{align}
$$

## Backward Propagation

Loss Function을 Neural Network의 출력값(a[L])으로 미분하면 다음과 같습니다. 여기서 a[4], a[L], y_hat은 모두 같은 표현입니다. 일반적으로 Neural Network의 최종 출력 값을 y_hat으로 표현합니다. a[L]는 마지막 레이어의 활성함수 출력값입니다. 본문에서 예제로 사용하는 Neural Network은 4개 레이어로 구성되어있기 때문에 a[4]와 a[L], y_hat은 모두 마지막 레이어의 출력을 의미합니다.

>$$
\begin{align}
\frac{\partial L(a^{\[L\]} , y)}{\partial a^{\[L\]} } & = \frac{-y}{a^{\[L\]}} - \frac{1-y}{1-a^{\[L\]} }(1-a^{\[L\]})' \\\\\
& = \frac{-y}{a^{\[L\]} } - \frac{1-y}{1-a^{\[L\]}}(-1) \\\\\
& = \frac{-y}{a^{\[L\]} } + \frac{1-y}{1-a^{\[L\]} } \\\\\
da^{\[L\]} & = \frac{-y}{a^{\[L\]} } + \frac{1-y}{1-a^{\[L\]} }
\end{align}
$$

{{< img src="https://taewanmerepo.github.io/2017/12/backpropatation/backprop01.jpg"
title="그림 2"
caption="Backward Propagation의 Input: Layer 4의 입력" >}}

Backward Propagation에서 첫 번째 Layer 4의 입력은 da[4]입니다. da[4]의 입력을 편미분하여 dw[4]와 db[4]를 구할 수 있습니다. dw[4]와 db[4]를 구하기 위한 사전 준비는 다음과 같습니다.

>$$
\begin{align}
a^{[^l]} & = g^{\[l\]}(Z^{\[l\]}) \\\\\
z^{\[l\]} & = W^{\[l\]}a^{\[l-1\]} + b^{\[l\]} \\\\\
\frac{\partial a^{[^l]}}{\partial z^{\[l\]}} & = \frac{\partial}{\partial z}g^{\[l\]}(z^{\[l\]}) = g^{\[l\]'}(z^{\[l\]}) \\\\\
\frac{\partial z^{\[^l\]}}{\partial W^{\[l\]}} & = \frac{\partial}{\partial W^{\[l\]}}(W^{\[l\]}a^{\[l-1\]} + b^{\[l\]}) = a^{\[l-1\]} \\\\\
\frac{\partial z^{\[^l\]}}{\partial b^{\[l\]}} & = \frac{\partial}{\partial b^{\[l\]}}(W^{\[l\]}a^{\[l-1\]} + b^{\[l\]}) = 1
\end{align}
$$

dw[4]와 db[4]는 다음과 같습니다.

>$$
\begin{align}
\frac{\partial L(a^{\[4\]}, y)}{\partial z^{\[4\]}} & =  \frac{\partial L(a^{\[4\]}, y)}{\partial a^{\[4\]}}\frac{\partial a^{\[4\]}}{\partial z^{\[4\]}} \\\\\
& = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) \\\\\
dz^{\[4\]} & = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) \\\\\
\frac{\partial L(a^{\[4\]}, y)}{\partial W^{\[4\]}} & = \frac{\partial L(a^{\[4\]}, y)}{\partial a^{\[4\]}}\frac{\partial a^{\[4\]}}{\partial z^{\[4\]}}\frac{\partial z^{\[4\]}}{\partial W^{\[4\]}} \\\\\
& = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) * a^{\[3\]} \\\\\
& = dz^{\[4\]} * a^{\[3\]} \\\\\
\frac{\partial L(a^{\[4\]}, y)}{\partial b^{\[4\]}} & = \frac{\partial L(a^{\[4\]}, y)}{\partial a^{\[4\]}}\frac{\partial a^{\[4\]}}{\partial z^{\[4\]}}\frac{\partial z^{\[4\]}}{\partial b^{\[4\]}} \\\\\
& = da^{\[4\]} * g^{\[4\]'}(z^{\[4\]}) \\\\\
& = dz^{\[4\]}
\end{align}
$$




#### ㅇㅇㅇ

>$$
\begin{align}
& dZ^{\[L\]} = A^{\[L\]} - Y \\\\\
& dW^{\[L\]} = \frac{1}{m}dZ^{\[L\]}A^{\[L\]^T} \\\\\
& db^{\[L\]} = \frac{1}{m}np.sum(dZ^{\[L\]}, axis=1, keepdims=True) \\\\\
& dZ^{\[L-1\]} = dW^{\[L\]^T}dZ^{\[L\]}g'^{\[L\]}(Z^{\[L\]}) \\\\\
& .... \\\\\
& dZ^{\[1\]} = dW^{\[L\]^T}dZ^{\[2\]}g'^{\[1\]}(Z^{\[1\]}) \\\\\
& dW^{\[1\]} = \frac{1}{m}dZ^{\[1\]}A^{\[1\]^T} \\\\\
& db^{\[1\]} = \frac{1}{m}np.sum(dZ^{\[1\]}, axis=1, keepdims=True)  
\end{align}
$$

## 참고자료
- [Coursera: Neural Networks and Deep Learning by deeplearning.ai, 4Week: Deep Neural Networks - Getting your matrix dimensions right](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right)
